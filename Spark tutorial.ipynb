{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459b786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51414f",
   "metadata": {},
   "source": [
    "# Lesson 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21b59522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3ab3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "age = [10,20,30]\n",
    "name = ['kris','asss', 'adasd']\n",
    "exper = [10,6,4]\n",
    "# Create the pandas DataFrame with column name is provided explicitly\n",
    "df = pd.DataFrame(name, columns=['name'])\n",
    "df['age'] = age\n",
    "df['experience'] = exper\n",
    "df.to_csv('test.csv',index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db198afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff2f5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6efebbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the spark session \n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dade03d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.167:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3c97302e20>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95f179b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fad754e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c7faa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "|  _c0|_c1|       _c2|\n",
      "+-----+---+----------+\n",
      "| name|age|experience|\n",
      "| kris| 10|        10|\n",
      "| asss| 20|         6|\n",
      "|adasd| 30|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f073777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|experience|\n",
      "+-----+---+----------+\n",
      "| kris| 10|        10|\n",
      "| asss| 20|         6|\n",
      "|adasd| 30|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('test.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0c46586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae6b5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f3536",
   "metadata": {},
   "source": [
    "# Lesson 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a144c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset \n",
    "\n",
    "df_pyspark = spark.read.option('header','true').csv('test.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8160749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f04b319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferSchema=True for reading data as string or integers\n",
    "# false is the defult where it reads dataset as string \n",
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f3c0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|experience|\n",
      "+-----+---+----------+\n",
      "| kris| 10|        10|\n",
      "| asss| 20|         6|\n",
      "|adasd| 30|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b75017af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff6cd8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('name') #.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c10615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|experience|\n",
      "+-----+----------+\n",
      "| kris|        10|\n",
      "| asss|         6|\n",
      "|adasd|         4|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['name', 'experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b139274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "554f1035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('experience', 'int')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94d2dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+------------------+\n",
      "|summary| name| age|        experience|\n",
      "+-------+-----+----+------------------+\n",
      "|  count|    3|   3|                 3|\n",
      "|   mean| null|20.0| 6.666666666666667|\n",
      "| stddev| null|10.0|3.0550504633038935|\n",
      "|    min|adasd|  10|                 4|\n",
      "|    max| kris|  30|                10|\n",
      "+-------+-----+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bcee2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------------------------+\n",
      "| name|age|experience|experience after 2 years|\n",
      "+-----+---+----------+------------------------+\n",
      "| kris| 10|        10|                      12|\n",
      "| asss| 20|         6|                       8|\n",
      "|adasd| 30|         4|                       6|\n",
      "+-----+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add colummn in a df \n",
    "df_pyspark = df_pyspark.withColumn('experience after 2 years', df_pyspark['experience']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e3020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5662978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| name|age|experience|\n",
      "+-----+---+----------+\n",
      "| kris| 10|        10|\n",
      "| asss| 20|         6|\n",
      "|adasd| 30|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Delete column \n",
    "df_pyspark = df_pyspark.drop('experience after 2 years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75e2aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|experience|\n",
      "+-----+---+----------+\n",
      "| kris| 10|        10|\n",
      "| asss| 20|         6|\n",
      "|adasd| 30|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns \n",
    "df_pyspark = df_pyspark.withColumnRenamed('name', 'Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3cf831",
   "metadata": {},
   "source": [
    "# Lesson 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6af8614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b224be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|Salery|\n",
      "+-----+----+----------+------+\n",
      "| kris|  10|        10|  1000|\n",
      "| asss|  20|         6|  2000|\n",
      "|adasd|  30|         4|  3000|\n",
      "| suny|  43|        11|  4000|\n",
      "| sdds|  23|         3|  5000|\n",
      "| hasr|  32|         4|  6000|\n",
      "|   dd|null|      null|  7000|\n",
      "| null|  23|         1|  8000|\n",
      "| null|  36|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6af38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b0c65697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "| kris| 10|        10|  1000|\n",
      "| asss| 20|         6|  2000|\n",
      "|adasd| 30|         4|  3000|\n",
      "| suny| 43|        11|  4000|\n",
      "| sdds| 23|         3|  5000|\n",
      "| hasr| 32|         4|  6000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9262866f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|Salery|\n",
      "+-----+----+----------+------+\n",
      "| kris|  10|        10|  1000|\n",
      "| asss|  20|         6|  2000|\n",
      "|adasd|  30|         4|  3000|\n",
      "| suny|  43|        11|  4000|\n",
      "| sdds|  23|         3|  5000|\n",
      "| hasr|  32|         4|  6000|\n",
      "|   dd|null|      null|  7000|\n",
      "| null|  23|         1|  8000|\n",
      "| null|  36|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## how =  all if null is for the entire raw \n",
    "## how =  any if null is exist in any column\n",
    "## how =any + threshold if null existi in + columns than threshold \n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d238bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "| kris| 10|        10|  1000|\n",
      "| asss| 20|         6|  2000|\n",
      "|adasd| 30|         4|  3000|\n",
      "| suny| 43|        11|  4000|\n",
      "| sdds| 23|         3|  5000|\n",
      "| hasr| 32|         4|  6000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f636a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|Salery|\n",
      "+-----+----+----------+------+\n",
      "| kris|  10|        10|  1000|\n",
      "| asss|  20|         6|  2000|\n",
      "|adasd|  30|         4|  3000|\n",
      "| suny|  43|        11|  4000|\n",
      "| sdds|  23|         3|  5000|\n",
      "| hasr|  32|         4|  6000|\n",
      "|   dd|null|      null|  7000|\n",
      "| null|  23|         1|  8000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how='any',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "26c91d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|experience|Salery|\n",
      "+-------+----+----------+------+\n",
      "|   kris|  10|        10|  1000|\n",
      "|   asss|  20|         6|  2000|\n",
      "|  adasd|  30|         4|  3000|\n",
      "|   suny|  43|        11|  4000|\n",
      "|   sdds|  23|         3|  5000|\n",
      "|   hasr|  32|         4|  6000|\n",
      "|     dd|null|      null|  7000|\n",
      "|missing|  23|         1|  8000|\n",
      "|missing|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.fillna('missing','name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d036e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill with mean \n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'experience', 'Salery'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'experience', 'Salery']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be942f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "| name| age|experience|Salery|age_imputed|experience_imputed|Salery_imputed|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "| kris|  10|        10|  1000|         10|                10|          1000|\n",
      "| asss|  20|         6|  2000|         20|                 6|          2000|\n",
      "|adasd|  30|         4|  3000|         30|                 4|          3000|\n",
      "| suny|  43|        11|  4000|         43|                11|          4000|\n",
      "| sdds|  23|         3|  5000|         23|                 3|          5000|\n",
      "| hasr|  32|         4|  6000|         32|                 4|          6000|\n",
      "|   dd|null|      null|  7000|         23|                 4|          7000|\n",
      "| null|  23|         1|  8000|         23|                 1|          8000|\n",
      "| null|  36|      null|  null|         36|                 4|          4000|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7019c42",
   "metadata": {},
   "source": [
    "# Filter Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d74c4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "| kris| 10|        10|  1000|\n",
      "| asss| 20|         6|  2000|\n",
      "|adasd| 30|         4|  3000|\n",
      "| suny| 43|        11|  4000|\n",
      "| sdds| 23|         3|  5000|\n",
      "| hasr| 32|         4|  6000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "924fff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "| kris| 10|        10|  1000|\n",
      "| asss| 20|         6|  2000|\n",
      "|adasd| 30|         4|  3000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of less than 3000\n",
    "df_pyspark.filter('salery<=3000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "46c0a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| kris| 10|\n",
      "| asss| 20|\n",
      "|adasd| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('salery<=3000').select(['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b0c5f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|name|age|experience|Salery|\n",
      "+----+---+----------+------+\n",
      "|kris| 10|        10|  1000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['salery']<=3000) & (df_pyspark['age']< 20)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cdb32",
   "metadata": {},
   "source": [
    "# GroupBy and Aggrigate Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8bf6917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------------+------+\n",
      "| name|age|  Department|Salery|\n",
      "+-----+---+------------+------+\n",
      "| kris| 10|         IOT|  1000|\n",
      "| asss| 20|    Big data|  2000|\n",
      "|adasd| 30|          ML|  3000|\n",
      "| suny| 43|         IOT|  4000|\n",
      "| sdds| 23|    Big data|  5000|\n",
      "| hasr| 32|Data science|  6000|\n",
      "| kris| 10|         IOT|  1000|\n",
      "| asss| 20|    Big data|  2000|\n",
      "|adasd| 30|          ML|  3000|\n",
      "| suny| 43|         IOT|  4000|\n",
      "| sdds| 23|    Big data|  5000|\n",
      "| hasr| 32|Data science|  6000|\n",
      "+-----+---+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9242e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+\n",
      "| name|sum(age)|sum(Salery)|\n",
      "+-----+--------+-----------+\n",
      "| asss|      40|       4000|\n",
      "| sdds|      46|      10000|\n",
      "| hasr|      64|      12000|\n",
      "| suny|      86|       8000|\n",
      "|adasd|      60|       6000|\n",
      "| kris|      20|       2000|\n",
      "+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby \n",
    "df_pyspark.groupBy('name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "57bebc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----------+\n",
      "|  Department|avg(age)|avg(Salery)|\n",
      "+------------+--------+-----------+\n",
      "|         IOT|    26.5|     2500.0|\n",
      "|Data science|    32.0|     6000.0|\n",
      "|          ML|    30.0|     3000.0|\n",
      "|    Big data|    21.5|     3500.0|\n",
      "+------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2bd9c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    4|\n",
      "|Data science|    2|\n",
      "|          ML|    2|\n",
      "|    Big data|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6ccd656e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salery)|\n",
      "+-----------+\n",
      "|      42000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'salery':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b267c49",
   "metadata": {},
   "source": [
    "# Training ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7128dd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|Salery|\n",
      "+-----+---+----------+------+\n",
      "| kris| 22|         1|  1000|\n",
      "| asss| 20|         3|  2000|\n",
      "|adasd| 30|         4|  3000|\n",
      "| suny| 43|         5|  4000|\n",
      "| sdds| 23|         6|  5000|\n",
      "| hasr| 32|         7|  6000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0c1d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- Salery: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dafff22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience', 'Salery']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "36c264db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"age\",\"experience\"],outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "71d17d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=featureassembler.transform(df_pyspark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3e59dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+--------------------+\n",
      "| name|age|experience|Salery|Independent Features|\n",
      "+-----+---+----------+------+--------------------+\n",
      "| kris| 22|         1|  1000|          [22.0,1.0]|\n",
      "| asss| 20|         3|  2000|          [20.0,3.0]|\n",
      "|adasd| 30|         4|  3000|          [30.0,4.0]|\n",
      "| suny| 43|         5|  4000|          [43.0,5.0]|\n",
      "| sdds| 23|         6|  5000|          [23.0,6.0]|\n",
      "| hasr| 32|         7|  6000|          [32.0,7.0]|\n",
      "+-----+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c5a33713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience', 'Salery', 'Independent Features']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "34ac22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data=output.select(\"Independent Features\",\"salery\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7c5a13d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|salery|\n",
      "+--------------------+------+\n",
      "|          [22.0,1.0]|  1000|\n",
      "|          [20.0,3.0]|  2000|\n",
      "|          [30.0,4.0]|  3000|\n",
      "|          [43.0,5.0]|  4000|\n",
      "|          [23.0,6.0]|  5000|\n",
      "|          [32.0,7.0]|  6000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cc2b7126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 15:52:15 WARN Instrumentation: [1abec157] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "##train test split\n",
    "train_data,test_data=finalized_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent Features', labelCol='salery')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1a74f922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.9767, 858.9149])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c4ec8654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-194.29115593103472"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057a1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
